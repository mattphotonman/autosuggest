{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from autosuggest import AutoSuggester, AutoSuggesterFitter, Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLTKCorpus(Corpus):\n",
    "    \n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for tokens in self.iter_tokens():\n",
    "            yield tokens\n",
    "        \n",
    "    def iter_tokens(self):\n",
    "        alpha_rgx = re.compile(\".*[a-z]+.*\")\n",
    "        sentence_terminators = {\".\", \"?\", \"!\"}\n",
    "        for text in self.texts:\n",
    "            tokens = []\n",
    "            for token in text:\n",
    "                token = token.lower()\n",
    "                if token in sentence_terminators:\n",
    "                    if tokens:\n",
    "                        yield tokens\n",
    "                    tokens = []\n",
    "                elif alpha_rgx.match(token):\n",
    "                    tokens.append(token)\n",
    "\n",
    "            if tokens:\n",
    "                yield tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    \"I am going to the\",\n",
    "    \"He is going to the\",\n",
    "    \"going to the\",\n",
    "    \"to the\",\n",
    "    \"the\",\n",
    "    \"we need\",\n",
    "    \"i think you should meet\",\n",
    "    \"should meet\",\n",
    "    \"meet\",\n",
    "    \"Do you have any\",\n",
    "    \"They all have different\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Out of Distribution Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_corpus = NLTKCorpus([text1])\n",
    "text2_corpus = NLTKCorpus([text2])\n",
    "text3_corpus = NLTKCorpus([text3])\n",
    "text4_corpus = NLTKCorpus([text4])\n",
    "text5_corpus = NLTKCorpus([text5])\n",
    "train_corpus = NLTKCorpus([text1, text2, text3, text4, text5])\n",
    "test_corpus = NLTKCorpus([text6, text7, text8, text9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = AutoSuggesterFitter(5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading folds\n",
      "getting grid\n",
      "getting grid values for n-grams of size 3\n",
      "getting grid values for n-grams of size 4\n",
      "getting grid values for n-grams of size 5\n",
      "Fitting 8 grid points\n",
      "weights = [1, 1, 1, 1]\n",
      "score = 0.18629198011141268\n",
      "weights = [1, 1, 1, 4469.0]\n",
      "score = 0.18646808921935928\n",
      "weights = [1, 1, 4469.0, 1]\n",
      "score = 0.1862736206897046\n",
      "weights = [1, 1, 4469.0, 4469.0]\n",
      "score = 0.18627006583278602\n",
      "weights = [1, 4469.0, 1, 1]\n",
      "score = 0.1850302903466768\n",
      "weights = [1, 4469.0, 1, 4469.0]\n",
      "score = 0.18509750755917803\n",
      "weights = [1, 4469.0, 4469.0, 1]\n",
      "score = 0.18499563364123423\n",
      "weights = [1, 4469.0, 4469.0, 4469.0]\n",
      "score = 0.1849748789910675\n"
     ]
    }
   ],
   "source": [
    "suggester = fitter.fit([text1_corpus, text2_corpus, text3_corpus, text4_corpus, text5_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 4469.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggester.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline training set score: 0.17130657332158974\n",
      "Training set score: 0.9308323541781792\n",
      "Baseline test set score: 0.15489645393938567\n",
      "Test set score: 0.18523769921880326\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline training set score: {}\".format(fitter.baseline_score(suggester, train_corpus)))\n",
    "print(\"Training set score: {}\".format(fitter.score(suggester, train_corpus)))\n",
    "print(\"Baseline test set score: {}\".format(fitter.baseline_score(suggester, test_corpus)))\n",
    "print(\"Test set score: {}\".format(fitter.score(suggester, test_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I am going to the\n",
      "Suggestions: [('world', 426), ('whale', 421), ('same', 388), ('people', 319), ('most', 273)]\n",
      "\n",
      "Sentence: He is going to the\n",
      "Suggestions: [('world', 426), ('whale', 421), ('same', 388), ('people', 319), ('most', 273)]\n",
      "\n",
      "Sentence: going to the\n",
      "Suggestions: [('world', 426), ('whale', 421), ('same', 388), ('people', 319), ('most', 273)]\n",
      "\n",
      "Sentence: to the\n",
      "Suggestions: [('world', 426), ('whale', 421), ('same', 388), ('people', 319), ('most', 273)]\n",
      "\n",
      "Sentence: the\n",
      "Suggestions: [('world', 410), ('whale', 407), ('same', 381), ('people', 299), ('most', 268)]\n",
      "\n",
      "Sentence: we need\n",
      "Suggestions: [('to', 24), ('not', 23), ('of', 16), ('a', 14), ('for', 7)]\n",
      "\n",
      "Sentence: i think you should meet\n",
      "Suggestions: [('the', 21), ('him', 11), ('you', 7), ('with', 6), ('them', 6)]\n",
      "\n",
      "Sentence: should meet\n",
      "Suggestions: [('the', 21), ('him', 11), ('you', 7), ('with', 6), ('them', 6)]\n",
      "\n",
      "Sentence: meet\n",
      "Suggestions: [('the', 21), ('him', 10), ('you', 7), ('with', 6), ('them', 6)]\n",
      "\n",
      "Sentence: Do you have any\n",
      "Suggestions: [('other', 85), ('thing', 68), ('one', 58), ('of', 43), ('more', 22)]\n",
      "\n",
      "Sentence: They all have different\n",
      "Suggestions: [('from', 7), ('times', 3), ('thing', 2), ('nature', 2), ('people', 2)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in test_examples:\n",
    "    print(\"Sentence: {}\".format(sentence))\n",
    "    print(\"Suggestions: {}\".format(suggester.suggest(sentence.lower().split(), 5)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test In Distribution Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLTKSampleCorpus(NLTKCorpus):\n",
    "    \n",
    "    def __init__(self, texts, hash_select_func):\n",
    "        self.hash_select_func = hash_select_func\n",
    "        super().__init__(texts)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for tokens in self.iter_tokens():\n",
    "            if self.hash_select_func(hash(tuple(tokens))):\n",
    "                yield tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = [text1, text2, text3, text4, text5, text6, text7, text8, text9]\n",
    "train_folds_corpi = [\n",
    "    NLTKSampleCorpus(all_texts, lambda h: 16 * i <= h % 100 < 16 * (i+1))\n",
    "    for i in range(5)\n",
    "]\n",
    "train_corpus = NLTKSampleCorpus(all_texts, lambda h: 0 <= h % 100 < 80)\n",
    "test_corpus = NLTKSampleCorpus(all_texts, lambda h: 80 <= h % 100 < 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = AutoSuggesterFitter(5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading folds\n",
      "getting grid\n",
      "getting grid values for n-grams of size 3\n",
      "getting grid values for n-grams of size 4\n",
      "getting grid values for n-grams of size 5\n",
      "Fitting 8 grid points\n",
      "weights = [1, 1, 1, 1]\n",
      "score = 0.6197858988556663\n",
      "weights = [1, 1, 1, 858.0]\n",
      "score = 0.9387043189368771\n",
      "weights = [1, 1, 858.0, 1]\n",
      "score = 0.952390180878553\n",
      "weights = [1, 1, 858.0, 858.0]\n",
      "score = 0.9545588778146918\n",
      "weights = [1, 858.0, 1, 1]\n",
      "score = 0.8769564414913253\n",
      "weights = [1, 858.0, 1, 858.0]\n",
      "score = 0.9193613879660392\n",
      "weights = [1, 858.0, 858.0, 1]\n",
      "score = 0.9198320413436691\n",
      "weights = [1, 858.0, 858.0, 858.0]\n",
      "score = 0.9380490956072352\n"
     ]
    }
   ],
   "source": [
    "suggester = fitter.fit(train_folds_corpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 858.0, 858.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggester.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline training set score: 0.16802194699733278\n",
      "Training set score: 0.3706936261580867\n",
      "Baseline test set score: 0.16669433836958325\n",
      "Test set score: 0.2216431216568372\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline training set score: {}\".format(fitter.baseline_score(suggester, train_corpus)))\n",
    "print(\"Training set score: {}\".format(fitter.score(suggester, train_corpus)))\n",
    "print(\"Baseline test set score: {}\".format(fitter.baseline_score(suggester, test_corpus)))\n",
    "print(\"Test set score: {}\".format(fitter.score(suggester, test_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I am going to the\n",
      "Suggestions: [('world', 380), ('same', 320), ('most', 310), ('other', 310), ('whale', 290)]\n",
      "\n",
      "Sentence: He is going to the\n",
      "Suggestions: [('world', 380), ('same', 320), ('most', 310), ('other', 310), ('whale', 290)]\n",
      "\n",
      "Sentence: going to the\n",
      "Suggestions: [('world', 380), ('same', 320), ('most', 310), ('other', 310), ('whale', 290)]\n",
      "\n",
      "Sentence: to the\n",
      "Suggestions: [('world', 380), ('same', 320), ('most', 310), ('other', 310), ('whale', 290)]\n",
      "\n",
      "Sentence: the\n",
      "Suggestions: [('world', 365), ('same', 310), ('most', 300), ('other', 280), ('whale', 275)]\n",
      "\n",
      "Sentence: we need\n",
      "Suggestions: [('not', 30), ('of', 20), ('to', 20), ('some', 15), ('a', 15)]\n",
      "\n",
      "Sentence: i think you should meet\n",
      "Suggestions: [('you', 15), ('with', 10), ('him', 10), ('the', 10), ('his', 5)]\n",
      "\n",
      "Sentence: should meet\n",
      "Suggestions: [('you', 15), ('with', 10), ('him', 10), ('the', 10), ('his', 5)]\n",
      "\n",
      "Sentence: meet\n",
      "Suggestions: [('you', 15), ('with', 10), ('him', 10), ('the', 10), ('his', 5)]\n",
      "\n",
      "Sentence: Do you have any\n",
      "Suggestions: [('other', 65), ('one', 55), ('thing', 55), ('of', 35), ('females', 20)]\n",
      "\n",
      "Sentence: They all have different\n",
      "Suggestions: [('from', 10), ('way', 5), ('ships', 5), ('manner', 5), ('sizes', 5)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in test_examples:\n",
    "    print(\"Sentence: {}\".format(sentence))\n",
    "    print(\"Suggestions: {}\".format(suggester.suggest(sentence.lower().split(), 5)))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
